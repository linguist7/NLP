{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"_8 순환 신경망(Recurrent Neural Network).ipynb의 사본","provenance":[{"file_id":"1PELxaO_9Pzov6svb0LlVLtCkNlTb_dFi","timestamp":1658240494196}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jacLnynuqUXZ"},"source":["# 순환 신경망 (Recurrent Neural Network, RNN)\n","\n","- **순서가 있는 데이터**를 입력으로 받음\n","\n","- 변화하는 입력에 대한 출력을 얻음\n","\n","- 시계열(날씨, 주가 등), 자연어와 같이 **시간의 흐름에 따라 변화하고, 그 변화가 의미를 갖는 데이터** "]},{"cell_type":"markdown","metadata":{"id":"3P0gpo4gqWUz"},"source":["## Feed Forward Network vs Recurrent Network\n","\n","- Feed Forward Net (앞먹임 구조)\n","  - 일반적인 구조의 신경망\n","\n","  - 입력 → 은닉 → 출력층 으로 이어지는 단방향 구조\n","\n","  - 이전 스텝의 출력의 영향을 받지 않음\n","\n","- Recurrent Net (되먹임 구조)\n","  - 이전 층(Layer), 또는 스텝의 출력이 다시 입력으로 연결되는 신경망 구조\n","\n","  - 각 스텝마다 이전 상태를 기억 시스템(Memory System)  \n","\n","  - 현재 상태가 이전 상태에 종속\n","\n","  <br>\n","\n","  <img src=\"https://www.researchgate.net/profile/Engin_Pekel/publication/315111480/figure/fig1/AS:472548166115333@1489675670530/Feed-forward-and-recurrent-ANN-architecture.png\">\n","\n","  <sub>출처: https://www.researchgate.net/figure/Feed-forward-and-recurrent-ANN-architecture_fig1_315111480</sub>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2h5HFH0BqYho"},"source":["## 순환 신경망 구조\n","\n","<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=\"700\">\n","\n","<br>\n","\n","- 입력 $x_t$에서 $t$는 시각을 뜻함\n","\n","- $X_0$에 대한 출력 $Y_0$이 다음 레이어에 전달\n","\n","- 각각의 입력에 대해 출력은 해당 레이어대로 출력값을 반환"]},{"cell_type":"markdown","metadata":{"id":"kowOGfSLqbSn"},"source":["## 순환 신경망의 다양한 구조\n","\n","<img src=\"https://static.packt-cdn.com/products/9781789346640/graphics/2d4a64ef-9cf9-4b4a-9049-cb9de7a07f89.png\">\n","  \n","  <sub>출처: https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789346640/11/ch11lvl1sec80/introduction</sub>\n","\n","- one to one\n","  - RNN\n","\n","- one to many\n","  - Image Captioning \n","\n","  - 이미지에 대한 설명 생성\n","\n","- many to one\n","  - Sentiment Classification\n","\n","  - 문장의 긍정/부정을 판단하는 감정 분석\n","\n","- many to many\n","  - Machine Translation\n","\n","  - 하나의 언어를 다른 언어로 번역하는 기계 번역\n","\n","- many to many\n","  - Video Classification(Frame Level)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uiIA4Ue7-G5B"},"source":["## 두 가지 정보(현재 입력, 이전 시각의 출력)을 처리하는 수식\n","$\\qquad h_t = tanh ( \\ h_{t-1} W_h \\ + \\ x_t W_x + b) $\n","\n","- $W_x$ : 입력 $x$를 출력 $h$로 변환하기 위한 가중치\n","\n","- $W_h$ : 다음 시각의 출력으로 변환하기 위한 가중치\n","\n","- $h$는 '상태'를 기억\n","\n","- $h_t \\ $를 은닉 상태(hidden state) 또는 은닉 상태 벡터(hidden state vector)라고도 불림\n","\n","  <sub>출처: https://colah.github.io/posts/2015-08-Understanding-LSTMs/</sub>"]},{"cell_type":"markdown","metadata":{"id":"lp3_sVIjHk0F"},"source":["## 순환 신경망 레이어 (RNN Layer)\n","\n","- 입력: `(timesteps, input_features)`\n","\n","- 출력: `(timesteps, output_features)`"]},{"cell_type":"code","metadata":{"id":"n08yr0aAIbFD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKIInEEZIcBj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bEZ8dQEEKxlQ"},"source":["## 케라스의 순환층\n","- `SimpleRNN` layer\n","\n","- 입력: `(batch_size, timesteps, input_features)`\n","\n","- 출력\n","  - `return_sequences`로 결정할 수 있음\n","  \n","  - 3D 텐서\n","    - 타임스텝의 출력을 모은 전체 시퀀스를 반환\n","\n","    - `(batch_size, timesteps, output_features)`\n","\n","  - 2D 텐서\n","    - 입력 시퀀스에 대한 마지막 출력만 반환\n","\n","    - `(batch_size, output_features)`\n"]},{"cell_type":"code","metadata":{"id":"lZ1YCi1iKMC8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQ9LbTgoKxIl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDxE5Jv0Kc7O"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-MDIz1zcIyP"},"source":["- 네트워크의 표현력을 증가시키기 위해 여러 개의 순환층을 차례대로 쌓는 것이 유용할 때가 있음\n","\n","  - 이런 설정에서는 중간층들이 전체 출력 시퀀스를 반환하도록 설정"]},{"cell_type":"code","metadata":{"id":"E1tUaIe2L7GZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j0CNAzN_deXE"},"source":["## IMDB 데이터 적용"]},{"cell_type":"markdown","metadata":{"id":"W1SbNYRdmVTY"},"source":["### 데이터 로드"]},{"cell_type":"code","metadata":{"id":"-27Tkihbcei2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KL-_piPIdms6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uf2EVnzceS4R"},"source":["### 모델 구성"]},{"cell_type":"code","metadata":{"id":"d7dfeDC5eMdj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnAUijOReWGu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pU_9-Hu7mcMa"},"source":["### 모델 학습"]},{"cell_type":"code","metadata":{"id":"XdPtzPwNeonA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afIyKdLsmj6N"},"source":["### 시각화"]},{"cell_type":"code","metadata":{"id":"sendhEujeu8S"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rjMFpiBe4Fa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLO526Sgf-U_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VTmZ_XiZguM5"},"source":["- 전체 시퀀스가 아니라 순서대로 500개의 단어만 입력했기 때문에 성능이 낮게 나옴\n","\n","- SimpleRNN은 긴 시퀀스를 처리하는데 적합하지 않음\n","\n","- SimpleRNN은 실전에 사용하기엔 너무 단순\n","\n","- SimpleRNN은 이론적으로 시간 $t$ 에서 이전의 모든 타임스텝의 정보를 유지할 수 있지만, 실제로는 긴 시간에 걸친 의존성은 학습할 수 없음\n","\n","- 그래디언트 소실 문제(vanishing gradient problem)\n","  - 이를 방지하기 위해 LSTM, GRU 같은 레이어 등장\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oZXJIdDai7sV"},"source":["# LSTM(Long Short-Term Memory)\n","- 장단기 메모리 알고리즘\n","\n","- 나중을 위해 정보를 저장함으로써 오래된 시그널이 점차 소실되는 것을 막아줌\n","\n","  <img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">\n","\n","  <sub>출처: https://colah.github.io/posts/2015-08-Understanding-LSTMs/</sub>"]},{"cell_type":"markdown","metadata":{"id":"j3afhQOUlqG9"},"source":["## IMDB 데이터"]},{"cell_type":"markdown","metadata":{"id":"tZOCK6EQl1QI"},"source":["### 데이터 로드"]},{"cell_type":"code","metadata":{"id":"00ZS9v3YmT_H"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UW-VG35_gwmt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uo8uhFyBl2k-"},"source":["### 모델 구성"]},{"cell_type":"code","metadata":{"id":"tFkAN2Dzl3GC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPr9Ec-El50W"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vAPnX6ACmenz"},"source":["### 모델 학습"]},{"cell_type":"code","metadata":{"id":"FGWhqX5GmNeR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gFiE5SpPmh_x"},"source":["### 시각화"]},{"cell_type":"code","metadata":{"id":"jwQ6ZYHTyXHS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NiZ6uiUPyXHN"},"source":["### 모델 평가"]},{"cell_type":"code","metadata":{"id":"DZjW6iOV3k-J"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dc0wuvArGHgD"},"source":["# GRU (Gated Recurrent Unit)\n","- LSTM을 더 단순하게 만든 구조\n","\n","- 기억 셀은 없고, 시간방향으로 전파하는 것은 은닉 상태만 있음\n","\n","- reset gate\n","  - 과거의 은닉 상태를 얼마나 무시할지 결정\n","\n","  - $r$ 값이 결정\n","\n","- update gate\n","  -  은닉 상태를 갱신하는 게이트  \n","\n","  - LSTM의 forget, input gate 역할을 동시에 함\n","  \n","  <img src=\"https://miro.medium.com/max/1400/1*jhi5uOm9PvZfmxvfaCektw.png\" width=\"500\">\n","\n","<sub>출처: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub>\n","\n","  ### $\\qquad z = \\sigma (x_t W^{(z)}_x + h_{t-1} W^{(z)}_h + b^{(z)} \\\\ \n","  \\qquad r = \\sigma (x_t W^{(r)}_x + h_{t-1} W^{(r)}_h + b^{(r)}) \\\\\n","  \\qquad \\tilde{i} = tanh (x_t W^{(i)}_x + (r \\odot h_{t-1}) W^{(i)}_h + b ) \\\\\n","  \\qquad h_t = (1 - z) \\odot h_{t-1} + z \\odot \\tilde{h}$\n"]},{"cell_type":"markdown","metadata":{"id":"TvnFLeqSsOSh"},"source":["## Reuters 데이터\n","\n","- IMDB와 유사한 데이터셋(텍스트 데이터)\n","\n","- 46개의 상호 배타적인 토픽으로 이루어진 데이터셋 \n","  - 다중 분류 문제\n"]},{"cell_type":"markdown","metadata":{"id":"ONPaMDFEsftq"},"source":["### 데이터셋 로드"]},{"cell_type":"code","metadata":{"id":"sFyvFVizsUVD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C234q3AVsUSp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VioYGD8Ms4zH"},"source":["### 데이터 전처리 및 확인"]},{"cell_type":"code","metadata":{"id":"2Sb0OxNlsUQT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNUkve6-sUN6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBxQZtOJsULl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6hwQtn_sUJN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_P_JxBFntQOP"},"source":["### 모델 구성\n","- LSTM 레이어도 SimpleRNN과 같이 `return_sequences` 인자 사용가능"]},{"cell_type":"code","metadata":{"id":"1vG3ZGSksUG5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K_uMdxmysUEg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjgupW2puHXA"},"source":["### 모델 학습"]},{"cell_type":"code","metadata":{"id":"T22YsRQ8sUBg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JGZhIiYeuSCD"},"source":["### 시각화"]},{"cell_type":"code","metadata":{"id":"INJu0-itsT_T"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLZLATDBuTtj"},"source":["### 모델 평가\n"]},{"cell_type":"code","metadata":{"id":"SVxVV-kmsT6D"},"source":[""],"execution_count":null,"outputs":[]}]}